import os
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from diffusers import UNet2DModel, DDPMScheduler, DDPMPipeline
from diffusers.optimization import get_cosine_schedule_with_warmup
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from tqdm.auto import tqdm

# --- é…ç½®ç±» ---
class Config:
    dataset_path = "dataset"         # å¢å¼ºåçš„æ•°æ®é›†è·¯å¾„
    output_dir = "lucky_clover_model" # æ¨¡å‹ä¿å­˜è·¯å¾„
    image_size = 128                # åˆ†è¾¨ç‡ 128x128
    
    # é’ˆå¯¹ 5090 (32GB) çš„è°ƒä¼˜
    train_batch_size = 128          # 128 å°ºå¯¸ä¸‹ 128 batch çº¦å  10-15G æ˜¾å­˜
    gradient_accumulation_steps = 1 # æ˜¾å­˜å……è¶³ï¼Œæ— éœ€ç´¯åŠ 
    learning_rate = 2e-4            # å¤§ Batch ä¸‹ç¨å¾®è°ƒé«˜å­¦ä¹ ç‡
    
    num_epochs = 200                # æ€»è®­ç»ƒè½®æ•°
    lr_warmup_steps = 500
    save_every_epochs = 10          # æ¯ 10 è½®ä¿å­˜ä¸€æ¬¡æƒé‡å¹¶å‡ºå›¾é¢„è§ˆ
    
    mixed_precision = "bf16"        # 5090 å¿…é€‰ bf16ï¼Œæ€§èƒ½å¥½ä¸”ç¨³å®š
    num_workers = 8                 # æ•°æ®è¯»å–çº¿ç¨‹æ•°

class CloverDataset(Dataset):
    def __init__(self, root, transform):
        # è¿‡æ»¤éå›¾ç‰‡æ–‡ä»¶
        self.images = [os.path.join(root, f) for f in os.listdir(root) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
        self.transform = transform
    def __len__(self): return len(self.images)
    def __getitem__(self, i):
        try:
            image = Image.open(self.images[i]).convert("RGB")
            return self.transform(image)
        except Exception as e:
            print(f"è¯»å–å›¾ç‰‡å‡ºé”™: {self.images[i]}, é”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªéšæœºå™ªå£°æ›¿ä»£ï¼Œé˜²æ­¢è®­ç»ƒä¸­æ–­
            return torch.randn(3, 128, 128)

def train():
    config = Config()
    
    # åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps
    )

    # å®šä¹‰é’ˆå¯¹ 128x128 ä¼˜åŒ–çš„ UNet
    model = UNet2DModel(
        sample_size=config.image_size,
        in_channels=3,
        out_channels=3,
        layers_per_block=2,
        # 128 åˆ†è¾¨ç‡ä¸‹ï¼Œä¸‹é‡‡æ · 5 æ¬¡åˆ° 4x4 æ˜¯æœ€ä½³å®è·µ
        block_out_channels=(128, 128, 256, 256, 512),
        down_block_types=(
            "DownBlock2D",      # 128 -> 64
            "DownBlock2D",      # 64 -> 32
            "DownBlock2D",      # 32 -> 16
            "AttnDownBlock2D",  # 16 -> 8 (ä¸­å±‚åŠ å…¥æ³¨æ„åŠ›æœºåˆ¶)
            "DownBlock2D",      # 8 -> 4
        ),
        up_block_types=(
            "UpBlock2D",        # 4 -> 8
            "AttnUpBlock2D",    # 8 -> 16
            "UpBlock2D",        # 16 -> 32
            "UpBlock2D",        # 32 -> 64
            "UpBlock2D",        # 64 -> 128
        ),
    )

    # ï¼ï¼é‡è¦ï¼ï¼é’ˆå¯¹ 5090 å…³é—­æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
    # model.enable_gradient_checkpointing() 

    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)

    preprocess = transforms.Compose([
        transforms.Resize((config.image_size, config.image_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]), # å½’ä¸€åŒ–åˆ° [-1, 1]
    ])
    
    dataset = CloverDataset(config.dataset_path, preprocess)
    train_dataloader = DataLoader(
        dataset, 
        batch_size=config.train_batch_size, 
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=True
    )

    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=config.lr_warmup_steps,
        num_training_steps=(len(train_dataloader) * config.num_epochs),
    )

    # å‡†å¤‡ç¯å¢ƒ
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    # åˆ›å»ºä¿å­˜ç›®å½•
    if accelerator.is_main_process:
        if not os.path.exists("samples"): os.makedirs("samples")
        if not os.path.exists(config.output_dir): os.makedirs(config.output_dir)

    print(f"å¼€å§‹è®­ç»ƒ... ç›®æ ‡è½®æ•°: {config.num_epochs}")

    for epoch in range(config.num_epochs):
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f"Epoch {epoch}")

        model.train()
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(model):
                clean_images = batch
                noise = torch.randn(clean_images.shape).to(clean_images.device)
                bs = clean_images.shape[0]
                
                # é‡‡æ ·éšæœºæ—¶é—´æ­¥
                timesteps = torch.randint(
                    0, noise_scheduler.config.num_train_timesteps, (bs,), 
                    device=clean_images.device
                ).long()
                
                # å‘å›¾ç‰‡æ·»åŠ å™ªå£°
                noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

                # é¢„æµ‹å™ªå£°
                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]
                loss = F.mse_loss(noise_pred, noise)
                
                accelerator.backward(loss)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            progress_bar.update(1)
            progress_bar.set_postfix(loss=loss.detach().item())
        
# --- å®šæœŸä¿å­˜å’Œé‡‡æ · ---
        if (epoch + 1) % config.save_every_epochs == 0:
            if accelerator.is_main_process:
                pipeline = DDPMPipeline(
                    unet=accelerator.unwrap_model(model), 
                    scheduler=noise_scheduler
                )
                
                # 1. ä¿å­˜é¢„è§ˆå›¾ (ç”±äº .gitignore æœ‰ /samples/ï¼Œè¿™é‡Œå¾ˆå®‰å…¨)
                print(f"\næ­£åœ¨ç”Ÿæˆ Epoch {epoch} é¢„è§ˆå›¾...")
                images = pipeline(batch_size=8, num_inference_steps=50).images
                for i, img in enumerate(images):
                    img.save(f"samples/epoch_{epoch+1}_{i}.png")
                
                # 2. ä¿å­˜å†å²æƒé‡ (å¸¦ç¼–å·)
                checkpoint_path = os.path.join(config.output_dir, f"checkpoint-epoch-{epoch+1}")
                pipeline.save_pretrained(checkpoint_path)
                
                # 3. ä¿å­˜æœ€æ–°æƒé‡åˆ° latest æ–‡ä»¶å¤¹
                latest_path = os.path.join(config.output_dir, "latest")
                pipeline.save_pretrained(latest_path)
                
                print(f"ğŸ‰ æ¨¡å‹å·²æ›´æ–°è‡³: {latest_path}")
                print(f"ğŸ’¾ å¤‡ä»½æƒé‡å·²ä¿å­˜: {checkpoint_path}")

    print("æ‰€æœ‰è®­ç»ƒä»»åŠ¡å·²å®Œæˆï¼")

if __name__ == "__main__":
    train()